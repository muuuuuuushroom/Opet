import gradio as gr
import torch
import argparse
from PIL import Image
from models import build_model
from engine_evon import DeNormalize

from util.custom_log import *
import os
import torchvision.transforms as standard_transforms
import cv2
import numpy as np
from util.misc import nested_tensor_from_tensor_list 
import zipfile
import tempfile
from pathlib import Path
import shutil
import pandas as pd

global_model = None
global_args = None
global_transform = None  
global_criterion = None

def load_model(cfg_path, device, ckpt_path):
    config = load_config(cfg_path)
    args = argparse.Namespace(**config)
    args.resume = ckpt_path
    model, criterion = build_model(args)
    
    if args.resume is None or not os.path.isfile(args.resume):
        raise FileNotFoundError(f"Checkpoint file not found: {args.resume}")
    
    checkpoint = torch.load(args.resume, map_location=device)
    model.load_state_dict(checkpoint['model'])
    model.to(device)
    model.eval()
    return model, args, criterion

def initialize_model(cfg_path, checkpoint_path):
    global global_model, global_args, global_transform, global_criterion
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    global_model, global_args, global_criterion = load_model(cfg_path, device, checkpoint_path)
    # dataset = build_soy(image_set='val', args=global_args)
    # global_transform = None  # dataset.transform
    print('Model and dataset loaded successfully')
    
def _maybe_free_infer_cuda_memory():
    """é‡Šæ”¾æ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸´æ—¶æ˜¾å­˜ç¼“å­˜ï¼›ä¸å½±å“å·²åŠ è½½çš„modelå¸¸é©»æ˜¾å­˜ã€‚"""
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
            # å¯é€‰ï¼šæ›´æ¿€è¿›çš„å›æ”¶ï¼ˆé€šå¸¸ä¸å¿…ï¼Œä½†ä½ å·²æœ‰åœ¨OOMé‡Œç”¨å®ƒï¼‰
            torch.cuda.ipc_collect()
        except Exception:
            pass

def visualization(samples, pred, vis_dir):
    """
    Visualize predictions
    """
    pil_to_tensor = standard_transforms.ToTensor()

    restore_transform = standard_transforms.Compose([
        DeNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        standard_transforms.ToPILImage()
    ])

    images = samples.tensors
    masks = samples.mask
    for idx in range(images.shape[0]):
        sample = restore_transform(images[idx])
        sample = pil_to_tensor(sample.convert('RGB')).numpy() * 255
        sample_vis = sample.transpose([1, 2, 0])[:, :, ::-1].astype(np.uint8).copy()
        h, w = sample_vis.shape[:2]
        # draw ground-truth points (red)
        size = 3
        # draw predictions (green)
        for p in pred[idx]:
            sample_vis = cv2.circle(sample_vis, (int(p[1]), int(p[0])), size, (0, 255, 0), -1)
            
        # save image
        if vis_dir is not None:
            # eliminate invalid area
            imgH, imgW = masks.shape[-2:]
            valid_area = torch.where(~masks[idx])
            valid_h, valid_w = valid_area[0][-1], valid_area[1][-1]
            sample_vis = sample_vis[:valid_h+1, :valid_w+1]

            cv2.imwrite(os.path.join(vis_dir, 'single/example.jpg'), sample_vis)

def _handle_oom(e: Exception, context: str):
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        except Exception:
            pass
    raise gr.Error(f"{context}ï¼šCUDA OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰ã€‚è¯·å°è¯•æ¢å°å›¾/å‡å°‘å¹¶å‘/æ”¹ç”¨CPUã€‚åŸå§‹ä¿¡æ¯ï¼š{e}")

def predict(image):
    """
    Gradio inputs: gr.Image(type="filepath") -> `image` is a filepath (str)
    """
    global global_model, global_args, global_transform, global_criterion
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # read image from filepath
    pil_img = Image.open(image).convert("RGB")

    pil_to_tensor = standard_transforms.ToTensor()
    tensor_image = pil_to_tensor(pil_img)

    # torchvision Normalize ImageNet
    normalize = standard_transforms.Normalize(
        mean=[0.485, 0.456, 0.406], 
        std=[0.229, 0.224, 0.225]
    )
    tensor_image = normalize(tensor_image).to(device) # [C,H,W], float

    samples = nested_tensor_from_tensor_list([tensor_image]).to(device)

    try:
        with torch.no_grad():
            outputs = global_model(samples, test=True, targets=None)
            outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]
            outputs_points = outputs['pred_points'][0]
            outputs_offsets = outputs['pred_offsets'][0]
            outputs_queries = outputs['points_queries']
        predict_cnt = len(outputs_scores)
        print('Total Counts:', predict_cnt)
        counts_text = f"è®¡æ•°å€¼ï¼š {predict_cnt}"
        
        # visualization
        vis_dir = "./visualizations_cache"
        os.makedirs(vis_dir, exist_ok=True)
        vis_path = os.path.join(vis_dir, "example.jpg")

        vis_bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
        pred_points = outputs.get("pred_points", None)
        
        img_h, img_w = samples.tensors.shape[-2:]
        if pred_points is not None:
            pts = pred_points[0]
            pts = [[pt[0]*img_h, pt[1]*img_w] for pt in pts] 
            if isinstance(pts, torch.Tensor):
                pts = pts.detach().cpu().numpy()
                
            for p in pts:
                vis_bgr = cv2.circle(vis_bgr, (int(p[1]), int(p[0])), 3, (0, 0, 255), -1)

        cv2.imwrite(vis_path, vis_bgr)
        return vis_path, counts_text
    except torch.cuda.OutOfMemoryError as e:
        _handle_oom(e, "å•å›¾æ¨ç†å¤±è´¥")
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            _handle_oom(e, "å•å›¾æ¨ç†å¤±è´¥")
        raise
    finally:
        # é‡Šæ”¾æœ¬æ¬¡æ¨ç†äº§ç”Ÿçš„ä¸´æ—¶æ˜¾å­˜ï¼ˆä¸åŠ¨global_modelï¼‰
        try:
            del samples, tensor_image
        except Exception:
            pass
        try:
            del outputs
        except Exception:
            pass
        try:
            del outputs_scores
        except Exception:
            pass
        _maybe_free_infer_cuda_memory()

def _count_from_pil(pil_img: Image.Image) -> int:
    global global_model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    pil_to_tensor = standard_transforms.ToTensor()
    tensor_image = pil_to_tensor(pil_img.convert("RGB"))

    normalize = standard_transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
    tensor_image = normalize(tensor_image).to(device)
    samples = nested_tensor_from_tensor_list([tensor_image]).to(device)

    outputs = None
    pts = None
    try:
        with torch.no_grad():
            outputs = global_model(samples, test=True, targets=None)
        img_h, img_w = samples.tensors.shape[-2:]
        pred_points = outputs.get("pred_points", None)
        if pred_points is not None:
            _pts = pred_points[0]
            pts = [[pt[0]*img_h, pt[1]*img_w] for pt in _pts]
        return outputs, pts
    finally:
        # åªæ¸…ç†è¾“å…¥/ä¸­é—´å˜é‡ï¼›outputsè¦è¿”å›ç»™ä¸Šå±‚å°±ä¸delå®ƒ
        try:
            del samples, tensor_image
        except Exception:
            pass
        _maybe_free_infer_cuda_memory()

def predict_zip(zip_path):
    """
    Gradio inputs: gr.File -> `zip_path` typically is a tempfile.NamedTemporaryFile-like.
    Returns:
      - excel filepath (.xlsx)
      - visualizations zip filepath (.zip)
      - dataframe rows: [[filename, count], ...]
    """
    if hasattr(zip_path, "name"):
        zip_path = zip_path.name

    if not isinstance(zip_path, str) or not os.path.isfile(zip_path):
        return None, None, []

    if not zip_path.lower().endswith(".zip"):
        return None, None, []

    exts = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff", ".webp"}

    base_vis_dir = Path("./visualizations_cache/from_zips")
    if base_vis_dir.exists():
        for p in base_vis_dir.iterdir():
            if p.is_file():
                p.unlink()
            elif p.is_dir():
                shutil.rmtree(p)
    base_vis_dir.mkdir(parents=True, exist_ok=True)

    rows = []
    total = 0
    oom_count = 0
    with tempfile.TemporaryDirectory(prefix="gr_zip_") as tmpdir:
        try:
            with zipfile.ZipFile(zip_path, "r") as zf:
                zf.extractall(tmpdir)
        except Exception as e:
            return None, None, []

        files = [p for p in Path(tmpdir).rglob("*") if p.is_file() and p.suffix.lower() in exts]
        files.sort(key=lambda p: str(p).lower())

        if not files:
            return None, None, []

        for p in files:
            outputs = None
            outputs_scores = None
            pil_img = None
            pts = None
            try:
                pil_img = Image.open(str(p)).convert("RGB")
                outputs, pts = _count_from_pil(pil_img)
                outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]

                cnt = int(len(outputs_scores))
                
                stem = Path(p.name).stem
                out_name = f"{stem}_pred{cnt}.jpg"
                out_path = base_vis_dir / out_name
                k = 1
                while out_path.exists():
                    out_name = f"{stem}_pred{cnt}_{k}.jpg"
                    out_path = base_vis_dir / out_name
                    k += 1
                
                # visualization
                vis_bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
                if pts is not None:
                    if isinstance(pts, torch.Tensor):
                        pts = pts.detach().cpu().numpy()
                    for pt in pts:
                        vis_bgr = cv2.circle(vis_bgr, (int(pt[1]), int(pt[0])), 3, (0, 0, 255), -1)
                cv2.imwrite(str(out_path), vis_bgr)

                rows.append([p.name, cnt])
                total += cnt
            except torch.cuda.OutOfMemoryError:
                oom_count += 1
                if torch.cuda.is_available():
                    try:
                        torch.cuda.empty_cache()
                        torch.cuda.ipc_collect()
                    except Exception:
                        pass
                rows.append([p.name, "OOM: æ˜¾å­˜ä¸è¶³"])
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    oom_count += 1
                    if torch.cuda.is_available():
                        try:
                            torch.cuda.empty_cache()
                            torch.cuda.ipc_collect()
                        except Exception:
                            pass
                    rows.append([p.name, "OOM: æ˜¾å­˜ä¸è¶³"])
                else:
                    rows.append([p.name, f"å¤±è´¥: {e}"])
            except Exception as e:
                rows.append([p.name, f"å¤±è´¥: {e}"])
            finally:
                # æ¯å¼ å›¾æ¨ç†ç»“æŸå°±æ¸…ç†ä¸€æ¬¡ï¼Œé¿å…æ‰¹é‡ç´¯ç§¯æ˜¾å­˜ç¢ç‰‡/ç¼“å­˜
                try:
                    del outputs_scores
                except Exception:
                    pass
                try:
                    del outputs
                except Exception:
                    pass
                _maybe_free_infer_cuda_memory()

    # Export Excel
    excel_path = str(base_vis_dir / "counts.xlsx")
    try:
        df = pd.DataFrame(rows, columns=["æ–‡ä»¶å", "è®¡æ•°/çŠ¶æ€"])
        df.to_excel(excel_path, index=False)
    except Exception as e:
        excel_path = None
        rows.append(["__export__", f"Excelå¯¼å‡ºå¤±è´¥: {e}"])

    # Export visualization zip
    vis_zip_path = str(base_vis_dir / "visualizations.zip")
    try:
        with zipfile.ZipFile(vis_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
            for imgp in sorted(base_vis_dir.glob("*.jpg")):
                zf.write(str(imgp), arcname=imgp.name)
    except Exception as e:
        vis_zip_path = None
        rows.append(["__export__", f"å¯è§†åŒ–æ‰“åŒ…å¤±è´¥: {e}"])

    return  excel_path, vis_zip_path, rows


USER_CREDENTIALS = {"admin": "654321"}

def check_login(username, password):
    """éªŒè¯ç™»å½•ä¿¡æ¯"""
    if username in USER_CREDENTIALS and USER_CREDENTIALS[username] == password:
        return True, "ç™»å½•æˆåŠŸï¼"
    else:
        return False, "ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯ï¼"

def gradio_demo():
    with gr.Blocks(title="å¤§è±†èƒå›Šè™«è®¡æ•°ç³»ç»Ÿ - è¯·å…ˆç™»å½•") as demo:
        # ç™»å½•ç•Œé¢ï¼ˆåˆå§‹æ˜¾ç¤ºï¼‰
        with gr.Column(visible=True, elem_id="login_section") as login_section:
            gr.Markdown("# ğŸ” å¤§è±†èƒå›Šè™«è®¡æ•°ç³»ç»Ÿ")
            gr.Markdown("### è¯·å…ˆç™»å½•ä»¥ä½¿ç”¨ç³»ç»Ÿ")
            
            with gr.Row():
                with gr.Column(scale=1):
                    username = gr.Textbox(
                        label="ç”¨æˆ·å", 
                        value="admin",  # é»˜è®¤ç”¨æˆ·å
                        placeholder="è¾“å…¥ç”¨æˆ·å",
                        scale=2
                    )
                with gr.Column(scale=1):
                    password = gr.Textbox(
                        label="å¯†ç ", 
                        type="password",
                        value="654321",  # é»˜è®¤å¯†ç 
                        placeholder="è¾“å…¥å¯†ç ",
                        scale=2
                    )
            
            with gr.Row():
                login_btn = gr.Button("ç™»å½•", variant="primary", size="lg")
                clear_btn = gr.Button("æ¸…é™¤", size="lg")
            
            login_status = gr.Textbox(label="ç™»å½•çŠ¶æ€", visible=False)
        
        # ä¸»åº”ç”¨ç•Œé¢ï¼ˆåˆå§‹éšè—ï¼‰
        with gr.Column(visible=False, elem_id="main_section") as main_section:
            gr.Markdown("# ğŸŒ± å¤§è±†èƒå›Šè™«è®¡æ•° Demo")
            
            with gr.Tab("å•å›¾ç²¾ç»†åŒ–ç‚¹å›å½’è®¡æ•°"):
                with gr.Row():
                    in_img = gr.Image(type="filepath", label="ä¸Šä¼ å›¾ç‰‡")
                with gr.Row():
                    clear_btn_main = gr.Button("æ¸…é™¤")
                    submit_btn = gr.Button("æäº¤", variant="primary")
                with gr.Row():
                    out_img = gr.Image(type="filepath", label="é¢„æµ‹ç»“æœ")
                with gr.Row():
                    out_txt = gr.Textbox(label="ç»Ÿè®¡ä¿¡æ¯", lines=1)
                
                submit_btn.click(fn=predict, inputs=[in_img], outputs=[out_img, out_txt])
                clear_btn_main.click(
                    fn=lambda: [None, None, None], 
                    inputs=None, 
                    outputs=[in_img, out_img, out_txt]
                )
            
            with gr.Tab("é«˜é€šé‡æ‰¹é‡å›¾åƒåˆ†æ"):
                zip_in = gr.File(label="ä¸Šä¼  .zip å‹ç¼©åŒ…æ–‡ä»¶", file_types=[".zip"])
                batch_btn = gr.Button("å¼€å§‹æ‰¹é‡è®¡æ•°", variant="primary")
                
                with gr.Row():
                    out_excel = gr.File(label="å¯¼å‡ºè®¡æ•°æŠ¥è¡¨")
                    out_viszip = gr.File(label="ä¸‹è½½æ‰€æœ‰å¯è§†åŒ–")
                
                out_table = gr.Dataframe(headers=["æ–‡ä»¶å", "è®¡æ•°/çŠ¶æ€"], label="ç»“æœ", wrap=True)
                batch_btn.click(
                    fn=predict_zip,
                    inputs=[zip_in],
                    outputs=[out_excel, out_viszip, out_table]
                )
            
            # æ·»åŠ é€€å‡ºç™»å½•æŒ‰é’®
            with gr.Row():
                logout_btn = gr.Button("é€€å‡ºç™»å½•", variant="secondary")
        
        # ç™»å½•æŒ‰é’®äº‹ä»¶
        def login_action(username, password):
            success, message = check_login(username, password)
            if success:
                return [
                    gr.update(visible=False),  # éšè—ç™»å½•ç•Œé¢
                    gr.update(visible=True),   # æ˜¾ç¤ºä¸»ç•Œé¢
                    gr.update(value=message, visible=True)
                ]
            else:
                try:
                    gr.Warning(message)
                except Exception:
                    pass

                return [
                    gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(value=message, visible=True)
                ]
        
        # æ¸…é™¤æŒ‰é’®äº‹ä»¶
        def clear_login():
            return [
                gr.update(value="admin"),
                gr.update(value="654321"),
                gr.update(visible=False)
            ]
        
        # é€€å‡ºç™»å½•äº‹ä»¶
        def logout_action():
            return [
                gr.update(visible=True),   # æ˜¾ç¤ºç™»å½•ç•Œé¢
                gr.update(visible=False),  # éšè—ä¸»ç•Œé¢
                gr.update(value="", visible=False)
            ]
        
        # ç»‘å®šäº‹ä»¶
        login_btn.click(
            fn=login_action,
            inputs=[username, password],
            outputs=[login_section, main_section, login_status]
        )
        
        clear_btn.click(
            fn=clear_login,
            inputs=None,
            outputs=[username, password, login_status]
        )
        
        logout_btn.click(
            fn=logout_action,
            inputs=None,
            outputs=[login_section, main_section, login_status]
        )
        
        # å›è½¦é”®ä¹Ÿå¯ä»¥è§¦å‘ç™»å½•
        password.submit(
            fn=login_action,
            inputs=[username, password],
            outputs=[login_section, main_section, login_status]
        )
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860, 
        share=False,
        show_error=True,
        debug=True
    )

if __name__ == "__main__":
    
    initialize_model(
        cfg_path='outputs/soy_newran/base_pet333/config.yaml', 
        checkpoint_path='outputs/soy_newran/base_pet333/best_checkpoint.pth'
    )
    # predict('data/soybean/images/24S2983-4-2.jpg')
    gradio_demo()